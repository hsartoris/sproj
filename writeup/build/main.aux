\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Biological Neural Networks}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Graph Structures}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Graph Locality}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Convolutional Neural Networks}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Adaptation to Graph Locality}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Concepts and Terms}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Adjacency Matrices}{3}}
\newlabel{subsec:adjacency}{{1.4.1}{3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Model}{4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{model}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Data}{4}}
\newlabel{sec:data}{{2.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Generation}{5}}
\newlabel{subsec:generation}{{2.1.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of 3-neuron network and adjacency matrix.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:toyex}{{2.1}{5}}
\@writefile{toc}{\contentsline {subsubsection}{Example Data Generation}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example output matrix for a 3-neuron network simulated for five steps.\relax }}{7}}
\newlabel{fig:exoutput}{{2.2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Restructuring}{7}}
\newlabel{subsec:restructuring}{{2.1.2}{7}}
\@writefile{toc}{\contentsline {subsubsection}{Input Data}{7}}
\newlabel{subfig:3outexgraph}{{2.3b}{8}}
\newlabel{sub@subfig:3outexgraph}{{b}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Transposed and truncated matrix and associated visualization.\relax }}{8}}
\newlabel{fig:data+vis}{{2.3}{8}}
\@writefile{toc}{\contentsline {subsubsection}{Target Data}{8}}
\newlabel{subsubsec:targetdata}{{2.1.2}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Generalizability}{8}}
\newlabel{subsec:hotswap}{{2.1.3}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Architecture}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Structure \& Computation Details}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality-defining Variables}{9}}
\@writefile{toc}{\contentsline {subsubsection}{Omitted Details}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Conceptual Model}{10}}
\newlabel{subsec:conceptualmodel}{{2.2.2}{10}}
\@writefile{toc}{\contentsline {subsubsection}{First Transition}{10}}
\@writefile{toc}{\contentsline {subsubsection}{Locality Layer}{11}}
\newlabel{subsubsec:locality}{{2.2.2}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Relationship between $\mathbb  {D}^{\prime }$ and $\mathbb  {D}^{\prime }_N$.\relax }}{11}}
\newlabel{fig:transform}{{2.4}{11}}
\newlabel{eq:ID}{{\bfseries  2.1a}{12}}
\newlabel{eq:IDOD}{{\bfseries  2.1b}{12}}
\newlabel{eq:dijO}{{\bfseries  2.1c}{12}}
\@writefile{toc}{\contentsline {subsubsection}{Final Transition}{13}}
\newlabel{eq:conceptfinal}{{\bfseries  2.2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Matrix Model}{13}}
\newlabel{subsec:matmodel}{{2.2.3}{13}}
\@writefile{toc}{\contentsline {subsubsection}{First Layer}{14}}
\newlabel{subsubsec:matfirstlayer}{{2.2.3}{14}}
\newlabel{eq:matfirst}{{\bfseries  2.3}{14}}
\@writefile{toc}{\contentsline {paragraph}{Example:}{14}}
\@writefile{toc}{\contentsline {subsubsection}{Locality Layer}{15}}
\newlabel{subsubsec:matconvlayer}{{2.2.3}{15}}
\newlabel{eq:matsecond}{{\bfseries  2.6}{16}}
\newlabel{eq:matsecondfinal}{{\bfseries  2.6b}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Final Layer}{16}}
\newlabel{subsubsec:matfinallayer}{{2.2.3}{16}}
\newlabel{eq:matfinal}{{\bfseries  2.7}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Benchmark Model}{16}}
\newlabel{subsec:benchmark}{{2.2.4}{16}}
\newlabel{eq:benchsecond}{{\bfseries  2.8}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}\textit  {n}-independence}{17}}
\newlabel{subsec:nindependence}{{2.2.5}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Trainable Values}{17}}
\@writefile{toc}{\contentsline {subparagraph}{First Layer}{17}}
\@writefile{toc}{\contentsline {subparagraph}{Locality Layer}{17}}
\@writefile{toc}{\contentsline {subparagraph}{Final Layer}{17}}
\@writefile{toc}{\contentsline {subparagraph}{Benchmark Model}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Implications}{18}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Training}{19}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Activation Functions}{19}}
\newlabel{sec:activation}{{3.1}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Initial \& Convolutional Layers}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces ReLU function definition and graph\relax }}{19}}
\newlabel{fig:relu}{{3.1}{19}}
\@writefile{toc}{\contentsline {subsubsection}{Alternative Activations}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Sigmoid function definition and graph\relax }}{20}}
\newlabel{fig:sigmoid}{{3.2}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Final Layer}{20}}
\newlabel{subsec:finalactivation}{{3.1.2}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Graph of $y=\qopname  \relax o{tanh}(x)$\relax }}{20}}
\newlabel{fig:tanh}{{3.3}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Loss \& Optimization}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Loss Function}{21}}
\newlabel{eq:ssd}{{\bfseries  3.1a}{21}}
\newlabel{eq:los}{{\bfseries  3.1b}{22}}
\newlabel{eq:losscalc}{{\bfseries  3.1}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Effects}{22}}
\newlabel{subsubsec:losseffects}{{3.2.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \linespread  {1.2}\selectfont  {}Example adjacency matrix\relax }}{22}}
\newlabel{fig:loss_ex}{{3.4}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Optimizer Function}{23}}
\newlabel{subsec:optimizer}{{3.2.2}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Adam decay function over 100 steps. Converges asymptotically to 1.\relax }}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Datasets}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Matrices}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Initialization}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Locality Layer Operations}{24}}
\newlabel{subsec:localops}{{3.4.2}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Hyperparameter Optimization}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Batch Size}{26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{results}{{4}{27}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Overfitting}{27}}
\newlabel{sec:overfitting}{{4.1}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \linespread  {1.2}\selectfont  {}Training parameters for null hypothesis networks\relax }}{27}}
\newlabel{fig:nullparams}{{4.1}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Empty Data}{27}}
\newlabel{subsec:empty}{{4.1.1}{27}}
\newlabel{subfig:empty_loss0}{{4.2a}{28}}
\newlabel{sub@subfig:empty_loss0}{{a}{28}}
\newlabel{subfig:empty_loss1}{{4.2b}{28}}
\newlabel{sub@subfig:empty_loss1}{{b}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Predictions and losses when training on an empty dataset\relax }}{28}}
\newlabel{fig:empty_loss}{{4.2}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Random Data}{28}}
\newlabel{subsec:random}{{4.1.2}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Average prediction for random data. loss: 0.5\relax }}{28}}
\newlabel{fig:random_output}{{4.3}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Analysis}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}3-neuron generator}{29}}
\newlabel{results_3neur}{{4.2}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Network structure and adjacency matrix of the generator. (Reproduced from Figure 2.1\hbox {})\relax }}{29}}
\newlabel{fig:2simplex+adjacency}{{4.4}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Example Model}{30}}
\newlabel{subsec:3neurex}{{4.2.1}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Caption for LOF}}{30}}
\newlabel{fig:3neur_loss+params}{{4.1}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Trained Network Operation}{31}}
\newlabel{subsec:trainedoperation}{{4.2.2}{31}}
\newlabel{subfig:3neur_in}{{4.5a}{31}}
\newlabel{sub@subfig:3neur_in}{{a}{31}}
\newlabel{subfig:3neur_out1}{{4.5c}{31}}
\newlabel{sub@subfig:3neur_out1}{{c}{31}}
\newlabel{subfig:3neur_outf}{{4.5d}{31}}
\newlabel{sub@subfig:3neur_outf}{{d}{31}}
\newlabel{subfig:3neur_pred}{{4.5e}{31}}
\newlabel{sub@subfig:3neur_pred}{{e}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Path of data through network. Transparency for each value is scaled relative to the maximum value found in the matrix.\relax }}{31}}
\newlabel{fig:3neur_run}{{4.5}{31}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis}{32}}
\@writefile{toc}{\contentsline {paragraph}{First Layer Functionality}{32}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional Layer Functionality}{32}}
\@writefile{toc}{\contentsline {paragraph}{Final Layer}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Final weights (max: 7.31)\relax }}{32}}
\newlabel{fig:3neur_flayer}{{4.6}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Higher-order Datasets}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Ten neuron generator and adjacency matrix. For purposes of clarity, all zero values in the matrix have been omitted.\relax }}{33}}
\newlabel{fig:10neur}{{4.7}{33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Applicability Beyond Training Data}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Example of data from generator defined in Figure 4.7\hbox {}, passed through the locality-based and benchmarks models.\relax }}{34}}
\newlabel{fig:samepred}{{4.8}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Inverted Network}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Loss \& parameters for model trained on data from generator given in Figure 4.7\hbox {}\relax }}{35}}
\newlabel{fig:9neur_loss+params}{{4.9}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Inverted version of Figure 4.4\hbox {}\relax }}{35}}
\newlabel{fig:2simplexVar1}{{4.10}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Cyclical Network}{35}}
\newlabel{subsec:cyclical}{{4.4.2}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Cyclical 3-neuron network\relax }}{36}}
\newlabel{fig:2simplexVar2}{{4.11}{36}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion}{37}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Potential Improvements}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Layering}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Locality Algorithm}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Loss}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Optimizer}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Potential Applications/Further Development}{38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Parameter Optimization Miscellanea}{39}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Data}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Spike Rate Determination}{39}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Model}{40}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Batched Architecture Calculations}{40}}
\newlabel{asec:batched}{{B.1}{40}}
