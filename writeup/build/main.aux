\relax 
\providecommand \oddpage@label [2]{}
\citation{Ray2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{neher1992patch}
\citation{Stosiek7319}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Biological Neural Networks}{3}}
\newlabel{sec:bioNN}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Neuron Behavior}{3}}
\citation{Xu8025}
\citation{networksciencebook}
\citation{networksciencebook}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Extracting Data}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Spike time raster plot\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:rasterplot}{{2.1}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Graphs}{4}}
\citation{networksciencebook}
\citation{Milo842}
\citation{netmotifs-robustness}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Digraph\relax }}{5}}
\newlabel{fig:digraph}{{2.2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Graph Structures in Biological Neural Networks}{5}}
\newlabel{subsec:motifs}{{2.2.1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces 3-simplex\relax }}{5}}
\newlabel{fig:3simplex}{{2.3}{5}}
\citation{Reimann2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Artificial Neural Networks}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Feedforward Network Operation}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Simple ANN\relax }}{6}}
\newlabel{fig:ANN}{{2.4}{6}}
\citation{2006mathematics}
\citation{lecun1998gradient}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {subsubsection}{Training}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Convolutional Neural Networks}{7}}
\newlabel{subsec:convnets}{{2.3.2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Graph Adjacency}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}General Operations \& Notation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Matrix Operations}{9}}
\newlabel{subsec:matops}{{2.5.1}{9}}
\@writefile{toc}{\contentsline {paragraph}{Concatenation}{9}}
\@writefile{toc}{\contentsline {paragraph}{Entrywise Product}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Adjacency Matrices}{10}}
\newlabel{subsec:adjacency}{{2.5.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Matrix Visualization}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The same matrix, in numerical and visual forms\relax }}{11}}
\newlabel{fig:matvisex}{{2.5}{11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Model}{12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{model}{{3}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data}{12}}
\newlabel{sec:data}{{3.1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Generation}{13}}
\newlabel{subsec:generation}{{3.1.1}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Example of 3-neuron network and adjacency matrix.\relax }}{13}}
\newlabel{fig:toyex}{{3.1}{13}}
\newlabel{eq:genstep}{{\bfseries  3.1}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Example Data Generation}{14}}
\newlabel{fig:exoutput}{{3.1.1}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Example output matrix for a 3-neuron network simulated for five steps.\relax }}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Restructuring}{15}}
\newlabel{subsec:restructuring}{{3.1.2}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Input Data}{15}}
\newlabel{subfig:genlform}{{3.3a}{16}}
\newlabel{sub@subfig:genlform}{{a}{16}}
\newlabel{subfig:truncdata}{{3.3b}{16}}
\newlabel{sub@subfig:truncdata}{{b}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Input data dimensionality\relax }}{16}}
\newlabel{fig:datadims}{{3.3}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Target Data}{16}}
\newlabel{subsubsec:targetdata}{{3.1.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Generalizability}{16}}
\newlabel{subsec:hotswap}{{3.1.3}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Architecture}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Structure \& Computation Details}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality-defining Variables}{17}}
\@writefile{toc}{\contentsline {subsubsection}{Omitted Details}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Conceptual Model}{18}}
\newlabel{subsec:conceptualmodel}{{3.2.2}{18}}
\@writefile{toc}{\contentsline {subsubsection}{First Transition}{18}}
\@writefile{toc}{\contentsline {subsubsection}{Locality Layer}{19}}
\newlabel{subsubsec:locality}{{3.2.2}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Relationship between $\mathbb  {D}^{\prime }$ and $\mathbb  {D}^{\prime }_N$.\relax }}{19}}
\newlabel{fig:transform}{{3.4}{19}}
\newlabel{eq:ID}{{\bfseries  3.3a}{20}}
\newlabel{eq:IDOD}{{\bfseries  3.3b}{20}}
\newlabel{eq:dijO}{{\bfseries  3.3c}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Final Transition}{21}}
\newlabel{eq:conceptfinal}{{\bfseries  3.4}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Matrix Model}{21}}
\newlabel{subsec:matmodel}{{3.2.3}{21}}
\@writefile{toc}{\contentsline {subsubsection}{First Layer}{21}}
\newlabel{subsubsec:matfirstlayer}{{3.2.3}{21}}
\newlabel{eq:matfirst}{{\bfseries  3.5}{22}}
\@writefile{toc}{\contentsline {paragraph}{Example:}{22}}
\@writefile{toc}{\contentsline {subsubsection}{Locality Layer}{23}}
\newlabel{subsubsec:matconvlayer}{{3.2.3}{23}}
\newlabel{eq:matsecond}{{\bfseries  3.8}{24}}
\newlabel{eq:matsecondfinal}{{\bfseries  3.8b}{24}}
\@writefile{toc}{\contentsline {subsubsection}{Final Layer}{24}}
\newlabel{subsubsec:matfinallayer}{{3.2.3}{24}}
\newlabel{eq:matfinal}{{\bfseries  3.9}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Benchmark Model}{24}}
\newlabel{subsec:benchmark}{{3.2.4}{24}}
\newlabel{eq:benchsecond}{{\bfseries  3.10}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}\textit  {n}-independence}{25}}
\newlabel{subsec:nindependence}{{3.2.5}{25}}
\@writefile{toc}{\contentsline {subsubsection}{Trainable Values}{25}}
\@writefile{toc}{\contentsline {subparagraph}{First Layer}{25}}
\@writefile{toc}{\contentsline {subparagraph}{Locality Layer}{25}}
\@writefile{toc}{\contentsline {subparagraph}{Final Layer}{25}}
\@writefile{toc}{\contentsline {subparagraph}{Benchmark Model}{25}}
\@writefile{toc}{\contentsline {subsubsection}{Implications}{26}}
\citation{nair2010rectified}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Training}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Activation Functions}{27}}
\newlabel{sec:activation}{{4.1}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Initial \& Convolutional Layers}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces ReLU function definition and graph\relax }}{27}}
\newlabel{fig:relu}{{4.1}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Final Layer}{27}}
\newlabel{subsec:finalactivation}{{4.1.2}{27}}
\citation{Rumelhart}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Graph of $y=\qopname  \relax o{tanh}(x)$\relax }}{28}}
\newlabel{fig:tanh}{{4.2}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Loss \& Optimization}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Loss Function}{29}}
\newlabel{eq:ssd}{{\bfseries  4.1a}{29}}
\newlabel{eq:los}{{\bfseries  4.1b}{29}}
\newlabel{eq:losscalc}{{\bfseries  4.1}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Effects}{29}}
\newlabel{subsubsec:losseffects}{{4.2.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \linespread  {1.2}\selectfont  {}Example adjacency matrix\relax }}{30}}
\newlabel{fig:loss_ex}{{4.3}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Optimizer Function}{30}}
\newlabel{subsec:optimizer}{{4.2.2}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Adam decay function over 100 steps. Converges asymptotically to 1.\relax }}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Datasets}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Matrices}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Initialization}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Locality Layer Operations}{32}}
\newlabel{subsec:localops}{{4.4.2}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Hyperparameter Optimization}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Batch Size}{33}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{results}{{5}{34}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Overfitting}{34}}
\newlabel{sec:overfitting}{{5.1}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \linespread  {1.2}\selectfont  {}Training parameters for null hypothesis networks\relax }}{34}}
\newlabel{fig:nullparams}{{5.1}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Empty Data}{34}}
\newlabel{subsec:empty}{{5.1.1}{34}}
\newlabel{subfig:empty_loss0}{{5.2a}{35}}
\newlabel{sub@subfig:empty_loss0}{{a}{35}}
\newlabel{subfig:empty_loss1}{{5.2b}{35}}
\newlabel{sub@subfig:empty_loss1}{{b}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Predictions and losses when training on an empty dataset\relax }}{35}}
\newlabel{fig:empty_loss}{{5.2}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Random Data}{35}}
\newlabel{subsec:random}{{5.1.2}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Average prediction for random data. loss: 0.5\relax }}{35}}
\newlabel{fig:random_output}{{5.3}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Analysis}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}3-neuron generator}{36}}
\newlabel{results_3neur}{{5.2}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Network structure and adjacency matrix of the generator. (Reproduced from Figure 3.1\hbox {})\relax }}{37}}
\newlabel{fig:2simplex+adjacency}{{5.4}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Example Model}{37}}
\newlabel{subsec:3neurex}{{5.2.1}{37}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Caption for LOF}}{37}}
\newlabel{fig:3neur_loss+params}{{5.1}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Trained Network Operation}{38}}
\newlabel{subsec:trainedoperation}{{5.2.2}{38}}
\newlabel{subfig:3neur_in}{{5.5a}{38}}
\newlabel{sub@subfig:3neur_in}{{a}{38}}
\newlabel{subfig:3neur_out1}{{5.5c}{38}}
\newlabel{sub@subfig:3neur_out1}{{c}{38}}
\newlabel{subfig:3neur_outf}{{5.5d}{38}}
\newlabel{sub@subfig:3neur_outf}{{d}{38}}
\newlabel{subfig:3neur_pred}{{5.5e}{38}}
\newlabel{sub@subfig:3neur_pred}{{e}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Path of data through network. Transparency for each value is scaled relative to the maximum value found in the matrix.\relax }}{38}}
\newlabel{fig:3neur_run}{{5.5}{38}}
\@writefile{toc}{\contentsline {subsubsection}{Brief Analysis}{39}}
\@writefile{toc}{\contentsline {paragraph}{Final Layer}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Final weights (max: 7.31)\relax }}{39}}
\newlabel{fig:3neur_flayer}{{5.6}{39}}
\@writefile{toc}{\contentsline {paragraph}{Locality Layer Functionality}{39}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Higher-order Datasets}{39}}
\newlabel{sec:localitybroken}{{5.3}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Ten neuron generator and adjacency matrix. For purposes of clarity, all zero values in the matrix have been omitted.\relax }}{40}}
\newlabel{fig:10neur}{{5.7}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Loss \& parameters for model trained on data from generator given in Figure 5.7\hbox {}\relax }}{40}}
\newlabel{fig:9neur_loss+params}{{5.8}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Example of data from generator defined in Figure 5.7\hbox {}, passed through the locality-based and benchmarks models.\relax }}{41}}
\newlabel{fig:samepred}{{5.9}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Applicability Beyond Training Data}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Model trained on 5.2\hbox {}}{42}}
\@writefile{toc}{\contentsline {subsubsection}{Inverted Network}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Inverted version of Figure 5.4\hbox {}\relax }}{42}}
\newlabel{fig:2simplexVar1}{{5.10}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces Data from Figure 5.10\hbox {}\relax }}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Cyclical 3-neuron network\relax }}{43}}
\newlabel{fig:2simplexVar2}{{5.12}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Cyclical Network}{43}}
\newlabel{subsec:cyclical}{{5.4.1}{43}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion}{44}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Data}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Complex Neurons}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Larger, Structured Networks}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Potential Improvements to Locality}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Layering}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Algorithm}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Loss}{46}}
\newlabel{subsec:lossdisc}{{6.2.3}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Optimizer}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Potential Applications/Further Development}{47}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{48}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Data}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1.1}Spike Rate Determination}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Batched Architecture Calculations}{48}}
\newlabel{asec:batched}{{A.2}{48}}
\bibstyle{abbrv}
\bibdata{total}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Full Run of Model 5.2.1\hbox {}}{49}}
\newlabel{asec:fullrun}{{A.3}{49}}
\bibcite{2006mathematics}{1}
\bibcite{networksciencebook}{2}
\bibcite{lecun1998gradient}{3}
\bibcite{neher1992patch}{4}
\bibcite{netmotifs-robustness}{5}
\bibcite{Ray2015}{6}
\bibcite{Reimann2017}{7}
\bibcite{Stosiek7319}{8}
\bibcite{Xu8025}{9}
