%! TEX root = /home/hsartoris/sproj/writeup/main.tex
\chapter{Discussion}

\section{Potential Improvements}

\subsection{Layering}
There may need to be more initial layers to provide useful data to the Locality 
layers.  As it stands, the model structure requires that the first layer both 
compare the activities of neuron pairs and format the resulting data in such a 
manner that the locality-based layer can usefully include it in determining node 
existence.  Adding at least one intermediary processing layer might allow the 
network to format the data going in to the locality layer in a more useful way.  
Merits further testing.

\subsection{Locality Algorithm}
Detail issues with summing inputs and outputs, and propose alternative algorithm 
here. Note issues with implementation.

\subsection{Loss}
As described in \ref{subsubsec:losseffects}, our custom loss function equally 
weights false positives and false negatives. Consider these cases:

\begin{enumerate}
	\item Output 0.3; target 1.0: adds $(0.3-1.0)^2=0.49$ to the loss
	\item Output 0.7; target 0.0: adds $(0.7-0.0)^2=0.49$ to the loss
\end{enumerate}
equivalent to the value added by an output of 0.7 when there should be 0. 

However, this latter case is the less correct of the two: 

\subsection{Optimizer}
I think it's in the paper about ELU or SELU that they use a ramping up learn 
rate and then decline. That might be ideal for the original, concatenation-based 
implemenation of the network, to avoid pushing it down the gradient too fast and 
zeroing out the convolutional matrix sections.

\section{Potential Applications/Further Development}

This shit is portable as fuck

Imagine an app(lication) that contains pretrained networks for different types 
of biological networks that researchers can then apply to their own data without 
having to interact at all with the nitty gritty bullshitty.

Community run database of trained networks as specific as submitters make them.
