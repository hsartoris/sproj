%! TEX root = /home/hsartoris/sproj/writeup/main.tex
\chapter{Model}
\label{model}
The model trained and tested here represents ... stuff

\section{Data}
\label{sec:data}
Insofar as we treat ANNs as providing arbitrary function approximation, training
a network requires input data representing the known data about the system we
wish to model, as well as output data we wish the network to produce from the
inputs. More generally, input data usually entails information that is easy to 
acquire about the process being modeled, while output data, or labels, 
correspond to a dataset that is difficult to acquire generally. Of course, this 
means that the first step in training a neural network is to assemble a 
sufficiently large set of inputs and outputs in order to fully, or at least 
approximately, characterize the problem at hand.

In our case, we wish to map from (relatively) easily available data about 
biological networks, individual neuron spike times, to network structure. While 
such data exist, generating our own allows us to better analyze the results of 
the algorithm.


\subsection{Generation}
In order to demonstrate the validity of our algorithm for graph convolution, we 
opt for a simplified form of the kind of data that would be used in a real-world 
setting.  To this end, we create adjacency matrices representing simple, 
small-\textit{n} toy networks.

FIG: 3 neuron model \& associated adjacency matrix

Binary values are used throughout these toy networks: either a connection exists 
or it doesn't; either a `neuron' is spiking or it isn't. To produce spiking 
data, we create an \textit{n}-vector $\mathbb{S}$ representing the current state 
of the toy network, with random neurons already spiking based on a chosen spike 
rate. From here, the process is as follows, where $\mathbb{M}$ is the adjacency 
matrix:

\[
	\underset{n \times n}{\mathbb{M}} \times \underset{n \times 1}{\mathbb{S}^t} 
	= \underset{n \times 1}{\mathbb{S}^{t+1}}
\]
Additonally, $\mathbb{S}^{t+1}$ may have one or more neurons spike randomly, as 
determined by the spike rate of the simulation.\footnote{SEE APPENDIX} All 
values are clipped to the range $[0,1]$, to avoid double spiking.

At each step, $\mathbb{S}$ is appended to an output matrix, which is saved after 
simulation is complete. For $t$ simulation steps, the completed output has shape 
$(n \times t)$.

APPENDIX BIT: have a section on how spike rates were determined for each 
network, as well as an example of producing data for the 3-neuron case.

\subsubsection{Generalizability}
\label{subsubsec:hotswap}
In most ANN implementations, feeding various data with the same label attached 
to it results in the network learning to ignore the input data and always spit 
out the desired label, rendering it useless. However, due to the unique 
structure of our model, this sort of overfitting is impossible (SEE SOME 
ARCHITECTURE SECTION). Therefore, we must merely construct a suitably 
representative generator network, meaning that it contains all of the 
inter-neuron relationships we expect to see in the data we ultimately feed in to 
test.

\subsection{Restructuring}
The model accepts data in the form of a spike-time raster plot of dimensions $(n 
\times t)$, where \textit{n} is the number of neurons and \textit{t} is the 
number of timesteps being considered. The axes are reversed in comparison to the 
data created by the generator, and thus in the process of loading in the spike 
trains we transpose the matrices to the expected dimensionality. Additionally, 
it is not always necessary to use the full number of steps generated, depending 
on the size of the generator network in question, as well as its spike rate. In 
such a scenario, we truncate the time dimension appropriately.

For a network accepting \textit{t} timesteps of data from \textit{n} neurons, 
the data fed into the network takes the following form:
\[ \begin{bmatrix}
		x_{11} & x_{12} & \dots & x_{1n}\\
		x_{21} & x_{22} & \dots & x_{2n}\\
		\vdots & \vdots & \ddots & \vdots\\
		x_{t1} & x_{t2} & \dots & x_{tn}
	\end{bmatrix} \]

\section{Architecture}
We will first describe the architecture in terms that, while accurate on the 
macro level, do not fully reflect the actual transformations occuring in the 
implemented model. We will then proceed to a mathematically representative 
version, leaving explanation of the batched version of the model to APPENDIX 
SECTION.

\subsection{First Transition}
To generate the first layer of the network, we inspect every pair of neurons in 
the input data. Since no pair of neurons is distinguishable from another, the 
comparison applied is the same in all cases: we apply the same convolutional 
filter to all pairs. We achieve this by concatenating the spike train of each 
neuron \textit{i} individually with every other neuron \textit{j}, then 
multiplying by a matrix $\mathbb{W}_0$ of dimensionality $(d \times 
2b)$.\footnote{Recall that the time dimension fed into the network is 
characterized by \textit{b}.}  Here \textit{d} is an arbitrary number that 
provides the network enough space to maintain information about each neuron pair 
through the subsequent transitions; we arrive at this value experimentally.

$\mathbb{W}_0$ is trained on, and thus the comparison of each pair of spike 
trains is left up to the network. The transition appears as follows, where 
$\mathbb{I}_x$ is the input column at \textit{x}:
\[
	\mathlarger\forall i,j \mid i,j < n: \underset{d \times 1}{d_{ij}} = 
	\underset{d \times 2b}{\mathbb{W}_0} \times 
	\left(\frac{\mathbb{I}_i}{\mathbb{I}_j}\right)
\]

This leaves us with $n^2$ \textit{d}-vectors, each characterizing one potential 
edge \textit{ij}.

\subsection{Convolutional Layer}
In this layer, we incorporate information from all nodes potentially adjacent to 
each edge \textit{ij}. From our previous layer, we have a matrix of shape $(d 
\times n^2)$ that we will refer to as $\mathbb{D}$, but it will be useful to 
keep in mind an alternate representation of that matrix, one in three 
dimensions, which we shall refer to as $\mathbb{D}^N$.

\begin{tikzpicture}
    \pgfmathsetmacro{\n}{20}
    \pgfmathsetmacro{\t}{40}
    \pgfmathsetmacro{\d}{12}
    \pgfmathsetmacro{\scale}{.07}
	\pic at (2,-1) {annotated cuboid={width=\n*4, height=3, depth=\d, xlabel=n, 
	ylabel=1, zlabel=d, scale=\scale}};
    \node at (3.7,-1) {\scalebox{2}{$\Leftrightarrow$}};
    \pic at (6.8,-.27) {annotated cuboid={width=\n, height=\n, depth=\d, xlabel=n, ylabel=n, zlabel=d, scale=\scale}};
\end{tikzpicture}\\

