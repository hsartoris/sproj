%! TEX root = /home/hsartoris/sproj/writeup/main.tex
\graphicspath{ {resources/models/3neurEx/9/} {resources/models/3neurEx/weights/} 
} 

\chapter{Results}
\label{results}
\section{3-neuron generator}
\label{results_3neur}
We first consider a generator network consisting of three nodes connected as in 
FIGURE HERE (should contain visual structure and adjacency matrix). All weights 
are binary, and a spike rate of .25 was used.\footnote{SEE APPENDIX	for 
information on spike rates}

\begin{table}[h]
	\centering
	\input{../resources/2simplex.tex}
	\hspace{2em}
	\begin{tabular}{l|lll}
		  & 0 & 1 & 2\\
		\hline
		0 & 0 & 0 & 0\\
		1 & 1 & 0 & 0\\
		2 & 1 & 1 & 0
	\end{tabular}
	\captionof{figure}{Network structure and adjacency matrix of the generator.}
	\label{fig:2simplex+adjacency}
\end{table}

While reconstructing a graph comprising only three nodes is not much of a feat, 
this simplified case allows us to demonstrate that our convolutional approach is 
capable of reconstruction at all. Furthermore, the small network size requires 
few timesteps and a small interlayer featurespace; i.e., $b,d<10$. This results 
in a relatively simple set of transitions, allowing us to explore and understand 
the inner workings of the network.

\subsection{Example Model}
\label{subsec:3neurex}
The following data are pulled from a model trained on data produced by the 
generator in figure \ref{fig:2simplex+adjacency}. Figure 
\ref{fig:3neur_loss+params} demonstrates the model's loss over time. In this 
example, \textit{b} and \textit{d} were pushed down in order to allow for better 
comprehension of the internal mechanics; the loss tends to converge more 
effectively and evenly given more computation power.
\input{../resources/models/3neurEx/plot.tex}

\subsubsection{Trained Network Operation}
Here, we examine in brief the internal operation of the trained model over a 
single input. For a complete look through the procedure of reconstruction for 
this network, please see APPENDIX.
\begin{figure}[h]
	\centering
	\begin{subfigure}{.15\textwidth}
		\centering
		\includegraphics[width=.75\textwidth]{input.png}
		\caption{Input}
	\end{subfigure}
	\hspace{1em}
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=\textwidth]{out0.png}
		\caption{Data after first transform}
	\end{subfigure}
	\hspace{1em}
	\begin{subfigure}{.3\textwidth}
		\includegraphics[width=\textwidth]{out1.png}
		\caption{Data after convolutional layer}
		\label{subfig:3neur_out1}
	\end{subfigure}
	\caption{Path of data through network, up to final transform}
	\label{fig:3neur_input}
\end{figure}

The last transformation of the network involves a matrix multiplication of the 
final layer weights\footnote{see appendix} with the data in 
\ref{subfig:3neur_out1}. This produces an adjacency matrix matching that of 
\ref{fig:2simplex+adjacency}.

\section{Applicability Beyond Training Data}
As described in \ref{subsubsec:hotswap}, the fact that our model is trained on 
data produced by only one generator is of little consequence; due to its 
structure, the only information it can learn is relational; i.e., 
per-neuron-pair. Consider the following examples, in which data was produced 
from several generator networks and fed into the model described in 
\ref{results_3neur}:
