#!/usr/bin/python3
"""
Simultaneously runs dumb & conv networks and logs both.

Usage: 
    runComp <dataDir> [--r=<runs>] [--s=<samples>] [--lr=<lr>]
        [--outDir=<outDir>] [--runId=<runId>] [--bs=<batchSize>]
        [--ts=<trainingSteps>] [--b=<b>] [--d=<d>] [--noDumb]
        [-i | --immediate] [--script]
    runComp -h | --help

Options:
    -h --help               Show this screen
    <dataDir>               Where to load inputs from. Assumes format dataStaging/!
    --r=<runs>              How many runs. Requires runId. [default: 1]
    --s=<samples>           How many samples to load from <dataDir> [default: 25000].
    --lr=<lr>               Init learning rate [default: .001].
    --outDir=<outDir>       Checkpoint & log directory. If not specified, leaves ckptDir to model/checkpoints/comp/; otherwise that + outDir
    --runId=<runId>         Specify runId. Defaults to timestamp.
    --bs=<batchSize>        Batch size [default: 32].
    --ts=<trainingSteps>    Max training steps [default: 20000].
    --b=<b>                 Time truncation [default: 30].
    --d=<d>                 Featurespace depth [default: 20].
    --noDumb                Doesn't run simple model.
    -i --immediate          Doesn't wait to confirm training parameters.
    --script                Running in a script. Sets arrow width to 79 instead 
    of autodetect.
"""

import warnings
from docopt import docopt
import numpy as np
import signal
import sys
import tensorflow as tf
import model.scripts.Prettify as Prettify
import math
import time
import shutil, os
from model.scripts.SeqData import seqData
from model.model import Model

ttyCols=79

arguments = docopt(__doc__)

script = arguments['--script']

pretty = Prettify.pretty((ttyCols if script else None))

runDumb = not arguments['--noDumb']

SAVE = True
# if you set this to False it will break
TBOARD_LOG = False

dataFile = "runData"
epochsToSave = 2 # directly dependent on batchsize of course

# default 32
batchSize = int(arguments['--bs'])

# default .001
initLearnRate = float(arguments['--lr'])

# default 20000
ts = arguments['--ts'].lower()
if 'k' in ts:
    trainingSteps = int(ts.split('k')[0]) * 1000
else:
    trainingSteps = int(ts)

# obvious default
ckptDir = "model/checkpoints/bench/"
if arguments['--outDir']: ckptDir += arguments['--outDir'] + "/"
if not os.path.exists(ckptDir):
    try: os.makedirs(ckptDir)
    except OSError as err:
        print("OSError while creating ckptDir " + ckptDir + ": {0}".format(err))


# default 25000
samples = arguments['--s'].lower()
if 'k' in samples:
    testMaxIdx = int(samples.split('k')[0]) * 1000
else:
    testMaxIdx = int(samples)

validMaxIdx = int(testMaxIdx * .9)
trainMaxIdx = int(validMaxIdx * .8)

# timesteps (default 30)
b = int(arguments['--b'])
# metalayers (default 20)
d = int(arguments['--d'])

prefix = "dataStaging/" + arguments['<dataDir>'] + "/"
if not os.path.exists(prefix):
    print("dataDir '" + prefix + "' not found, exiting.")
    sys.exit()

n = np.loadtxt(prefix + "struct", delimiter=',').shape[0]

runs = int(arguments['--r'])

if arguments['--runId'] is None:
    localtime = time.localtime()
    runId = str(localtime.tm_mday) + "_" + str(localtime.tm_hour) + str(localtime.tm_min)
else:
    runId = arguments['--runId']
    try:
        runId = int(runId)
    except ValueError:
        pass

if runs > 1 and type(runId) is str:
    warnings.warn("For multiple runs, runId must be an integer value. Setting 
    runs = 1.")
    runs = 1


training = None
validation = None
testing = None

def loadData():
    global training
    global validation
    global testing
    global trainMaxIdx
    global validMaxIdx
    global testMaxIdx
    # 5120, 6400, 8000
    # 1280, 1600, 2000
    cols = ttyCols if script else None
    print("Loading training data")
    training = seqData(0, trainMaxIdx, prefix, b, ttyCols=cols)
    trainMaxIdx -= training.crop(batchSize)
    print("Loading validation data")
    validation = seqData(trainMaxIdx, validMaxIdx, prefix, b, ttyCols=cols)
    validMaxIdx -= validation.crop(batchSize)
    print("Loading testing data")
    testing = seqData(validMaxIdx, testMaxIdx, prefix, b, ttyCols=cols)
    testMaxIdx -= testing.crop(batchSize)

def signal_handler(signal, frame):
    makePred()
    if (input("Exit? [Y/n]") or "Y") == "Y":
        cleanup()
        sess.close()
        sys.exit()
signal.signal(signal.SIGINT, signal_handler)

def cleanup():
    clean = input("Clean up logs and checkpoints? [Y/n]") or "Y"
    if clean == "Y":
        shutil.rmtree(ckptDir + runId)

def makePred(checkDir=None):
    global testing
    global sess
    if checkDir is not None:
        saver = tf.train.Saver()
        f = open(checkDir + "latest")
        saver.restore(sess, checkDir + f.readline() + ".ckpt")
        f.close()
        testing = seqData(0, 5, prefix, b)
        testData = testing.data
        testLabels = testing.labels
    testX, testY, _ = testing.next(1)
    if runDumb:
        print("Dumb model prediction:")
        print(np.round(sess.run(m_dumb.prediction, 
        feed_dict={_data: testX, _labels: testY})[0].reshape(n,n), 1))
    print("Conv model prediction:")
    print(np.round(sess.run(m_conv.prediction,
        feed_dict={_data: testX, _labels: testY})[0].reshape(n,n), 1))

def dumpData():
    out = "# b\n" + str(b) + "\n"
    out += "# d\n" + str(d) + "\n"
    out += "# n\n" + str(n) + "\n"
    out += "# batchSize\n" + str(batchSize) + "\n"
    out += "# initLearnRate\n" + str(initLearnRate) + "\n"
    out += "# trainingSteps\n" + str(trainingSteps) + "\n"
    out += "# prefix\n" + prefix + "\n"
    out += ("# testMaxIdx, validMaxIdx, trainMaxIdx\n" + str(testMaxIdx) + ", " +
            str(validMaxIdx) + ", " + str(trainMaxIdx) + "\n")
    out += "# noDumb\n" + str((0 if runDumb else 1)) + "\n"
    out += "# script\n" + str(script) + "\n"
    out += str(arguments) # just for good measure
    return out

def matrixMagnitude(mat, sess):
    return sess.run(tf.reduce_sum(tf.abs(mat)))

if not arguments['--immediate']:
    print(dumpData())
    cont = input("Continue with these parameters? [Y/n] ") or "Y"
    if not cont.lower() == "y":
        print("Exiting")
        sys.exit()


_data = tf.placeholder(tf.float32, [None, b, n])
_labels = tf.placeholder(tf.float32, [None, 1, n*n])

if runDumb:
    m_dumb = Model(b, d, n, _data, _labels, batchSize, 
        learnRate = initLearnRate, dumb=True)

m_conv = Model(b, d, n, _data, _labels, batchSize, learnRate = initLearnRate, dumb=False)

# save some data
if SAVE:
    saver = tf.train.Saver()
    saveDir = (ckptDir + runId + "/")
    try: os.mkdir(saveDir)
    except OSError as err:
        print(err)
        cont = input("Do you wish to overwrite and continue? [Y/n]") or "Y"
        if cont.upper() == "Y":
            shutil.rmtree(saveDir)
            os.mkdir(saveDir)
        else:
            print("exiting")
            sys.exit()
    if script:
        sys.stdout = open(saveDir + "stdout", "w+")
        sys.stderr = open(saveDir + "stderr", "w+")
    f = open(saveDir + dataFile, "w+")
    f.write(dumpData())
    f.close()
    f = open(saveDir + "prefix", "w+")
    f.write(prefix)
    f.close()


if TBOARD_LOG:
    lossSum = tf.summary.scalar("train_loss", m.loss)
    summWriter = tf.summary.FileWriter(ckptDir + runId + "/train")
    validWriter = tf.summary.FileWriter(ckptDir + runId + "/validation")

loadData()

init = tf.global_variables_initializer()
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
tf.logging.set_verbosity(tf.logging.ERROR)

#-----------------------------MODEL TRAINING------------------------------------

sess = tf.Session(config=config)
sess.run(init)

w0_tot = sess.run(tf.reduce_sum(tf.abs(m_conv.weights['layer0'])))
w1in_tot = sess.run(tf.reduce_sum(tf.abs(m_conv.weights['layer1'][0])))
w1out_tot = sess.run(tf.reduce_sum(tf.abs(m_conv.weights['layer1'][1])))
w1f_tot = sess.run(tf.reduce_sum(tf.abs(m_conv.weights['layer1'][2])))
wf_tot = sess.run(tf.reduce_sum(tf.abs(m_conv.weights['final'])))

if runDumb:
    dw0_tot = matrixMagnitude(m_dumb.weights['layer0'], sess)
    dw1_tot = matrixMagnitude(m_dumb.weights['layer1'][0], sess)
    dwf_tot = matrixMagnitude(m_dumb.weights['final'], sess)

f = open(saveDir + dataFile, "a")
f.write("Convolutional model matrix magnitudes:\n")
f.write("w0_tot: " + str(w0_tot) + "\n")
f.write("w1in_tot: " + str(w1in_tot) + "\n")
f.write("w1out_tot: " + str(w1out_tot) + "\n")
f.write("w1f_tot: " + str(w1f_tot) + "\n")
f.write("wf_tot: " + str(wf_tot) + "\n")
if runDumb:
    f.write("\nSimple model matrix magnitudes:\n")
    f.write("w0_tot: " + str(dw0_tot) + "\n")
    f.write("w1_tot: " + str(dw1_tot) + "\n")
    f.write("wf_tot: " + str(dwf_tot) + "\n")
f.close()


steps = []
if runDumb: 
    losses_dumb = []
    vlosses_dumb = []
losses_conv = []
vlosses_conv = []

epochCount = 0
for step in range(trainingSteps):
    batchX, batchY, batchId = training.next(batchSize)
    if runDumb:
        sess.run(m_dumb.optimize, feed_dict={_data: batchX, _labels: batchY})
    sess.run(m_conv.optimize, feed_dict={_data: batchX, _labels: batchY})
    if batchId == trainMaxIdx or batchId == int(trainMaxIdx/2) or step == 1:
        # end of epoch
        # calculate current loss on training data
        feed_dict = {_data: batchX, _labels: batchY}

        if runDumb:
            loss_dumb = sess.run(m_dumb.loss, feed_dict=feed_dict)
        loss_conv = sess.run(m_conv.loss, feed_dict=feed_dict)

        #loss_conv = sess.run([m_conv.loss], 
        #        feed_dict={_data: batchX, _labels: batchY})

        # calculate validation loss
        validX, validY, _ = validation.next(batchSize)
        feed_dict = {_data: validX, _labels:validY}
        if runDumb:
            vloss_dumb = sess.run(m_dumb.loss, feed_dict=feed_dict)
        vloss_conv = sess.run(m_conv.loss, feed_dict=feed_dict)

        #vloss_conv = sess.run([m_conv.loss], 
        #        feed_dict={_data: validX, _labels: validY})

        steps.append(step)
        if runDumb:
            losses_dumb.append(loss_dumb)
            vlosses_dumb.append(vloss_dumb)
        losses_conv.append(loss_conv)
        vlosses_conv.append(vloss_conv)

        if TBOARD_LOG:
            # log various data
             validWriter.add_summary(vloss, step)
             summWriter.add_summary(tLoss, step)
        if not step == 1: epochCount += 1
        print("Step " + str(step) + 
            ("\tdumb batch loss = {:.4f}".format(vloss_dumb) if runDumb else "") + 
            "; conv batch loss = {:.4f}".format(vloss_conv))

        if epochCount % epochsToSave == 0 and SAVE:
            save = saver.save(sess, saveDir + "/" + str(step) + ".ckpt")
            f = open(saveDir + "latest", "w+")
            f.write(str(step))
            f.close()
            print("Saved checkpoint " + str(step))

    pretty.arrow(batchId, trainMaxIdx)

if SAVE:
    save = saver.save(sess, saveDir + "final.ckpt")
    f = open(saveDir + "latest", "w+")
    f.write("final")
    f.close()

    print("Training complete; model saved in file %s" % save)
    if runDumb:
        minLossDumb = min(enumerate(vlosses_dumb), key = lambda t: t[1])
        minLossDumb = (steps[minLossDumb[0]], minLossDumb[1])
        print("Minimum dumb loss:", str(minLossDumb))

    minLossConv = min(enumerate(vlosses_conv), key = lambda t: t[1])
    minLossConv = (steps[minLossConv[0]], minLossConv[1])
    print("Minimum conv loss:", str(minLossConv))
    f = open(saveDir + dataFile, 'a')
    if runDumb: f.write("# min dumb loss\n" + str(minLossDumb) + "\n")
    f.write("# min conv loss\n" + str(minLossConv) + "\n")
    f.close()

makePred()

os.mkdir(ckptDir + runId + "/conv")
os.mkdir(ckptDir + runId + "/conv/mats")
m_conv.saveMats(ckptDir + runId + "/conv/mats/", sess)

if runDumb:
    os.mkdir(ckptDir + runId + "/dumb")
    os.mkdir(ckptDir + runId + "/dumb/mats")
    m_dumb.saveMats(ckptDir + runId + "/dumb/mats/", sess)
    np.savetxt(ckptDir + runId + "/dumb/losses", np.array(losses_dumb), delimiter=',')
    np.savetxt(ckptDir + runId + "/dumb/vlosses", np.array(vlosses_dumb), delimiter=',')

np.savetxt(ckptDir + runId + "/steps", np.array(steps), delimiter=',')
np.savetxt(ckptDir + runId + "/conv/losses", np.array(losses_conv), delimiter=',')
np.savetxt(ckptDir + runId + "/conv/vlosses", np.array(vlosses_conv), delimiter=',')
sess.close()
sys.stdout.close()
sys.stderr.close()
