#!/usr/bin/python3
import numpy as np
import signal
import sys
import tensorflow as tf
import model.scripts.Prettify as Prettify
import math
import time
import shutil, os
from model.scripts.SeqData import seqData
from model.model import Model

EXIT = False
SAVE_CKPT = True
# if you set this to False it will break
TBOARD_LOG = True

dataFile = "runData"
epochsToSave = 2 # directly dependent on batchsize of course
batchSize = 32
initLearnRate = .001
trainingSteps = 20000
ckptDir = "model/checkpoints/"
pretty = Prettify.pretty()
testMaxIdx = 25000
validMaxIdx = int(testMaxIdx * .9)
trainMaxIdx = int(validMaxIdx * .8)

# timesteps
b = 50
# metalayers
d = 30

if len(sys.argv) > 2 and sys.argv[1] == "run":
    prefix = "dataStaging/" + sys.argv[2] + "/"
    n = np.loadtxt(prefix + "struct.csv", delimiter=',').shape[0]

localtime = time.localtime()
runId = str(localtime.tm_mday) + "_" + str(localtime.tm_hour) + str(localtime.tm_min)

training = None
validation = None
testing = None

def loadData():
    global training
    global validation
    global testing
    global trainMaxIdx
    global validMaxIdx
    global testMaxIdx
    # 5120, 6400, 8000
    # 1280, 1600, 2000
    training = seqData(0, trainMaxIdx, prefix, b)
    trainMaxIdx -= training.crop(batchSize)
    validation = seqData(trainMaxIdx, validMaxIdx, prefix, b)
    validMaxIdx -= validation.crop(batchSize)
    testing = seqData(validMaxIdx, testMaxIdx, prefix, b)
    testMaxIdx -= testing.crop(batchSize)

def signal_handler(signal, frame):
    makePred()
    if (input("Exit? [Y/n]") or "Y") == "Y":
        cleanup()
        sess.close()
        sys.exit()
signal.signal(signal.SIGINT, signal_handler)

def cleanup():
    clean = input("Clean up logs and checkpoints? [Y/n]") or "Y"
    if clean == "Y":
        shutil.rmtree(ckptDir + runId)

def makePred(checkDir=None):
    global testing
    global sess
    if checkDir is not None:
        saver = tf.train.Saver()
        f = open(checkDir + "latest")
        saver.restore(sess, checkDir + f.readline() + ".ckpt")
        f.close()
        testing = seqData(0, 5, prefix, b)
        testData = testing.data
        testLabels = testing.labels
    testX, testY, _ = testing.next(1)
    print(np.round(sess.run(m.prediction, 
        feed_dict={_data: testX, _labels: testY})[0].reshape(n,n), 1))

# don't load data if given args
if len(sys.argv) == 1 or not sys.argv[1] == "prep": loadData()
# what on earth is prep lol

loadFrom = None
trainable = None
if len(sys.argv) > 2 and sys.argv[1] == "load": 
    loadFrom = ckptDir + sys.argv[2] + "/"
    if len(sys.argv) >= 4:
        trainArgs = sys.argv[3].split(',')
        trainable = [arg == 'T' for arg in trainArgs]
    if len(sys.argv) == 5:
        prefix = sys.argv[4] + "/"
        loadData()
_data = tf.placeholder(tf.float32, [None, b, n])
_labels = tf.placeholder(tf.float32, [None, 1, n*n])
m = Model(b, d, n, _data, _labels, batchSize, learnRate = initLearnRate, 
    matDir = loadFrom, trainable=trainable)


# save some data
if len(sys.argv) > 1 and sys.argv[1] == "pred":
    makePred(ckptDir + sys.argv[2] + "/")
    sys.exit()
elif SAVE_CKPT:
    saver = tf.train.Saver()
    saveDir = (ckptDir + runId + "/")
    os.mkdir(saveDir)
    f = open(saveDir + dataFile, "w+")
    f.write("# b\n" + str(b) + "\n")
    f.write("# d\n" + str(d) + "\n")
    f.write("# n\n" + str(n) + "\n")
    f.write("# batchSize\n" + str(batchSize) + "\n")
    f.write("# initLearnRate\n" + str(initLearnRate) + "\n")
    f.write("# trainingSteps\n" + str(trainingSteps) + "\n")
    f.write("# prefix\n" + prefix + "\n")
    f.write("# testMaxIdx, validMaxIdx, trainMaxIdx\n" + str(testMaxIdx) + ", " +
            str(validMaxIdx) + ", " + str(trainMaxIdx) + "\n")
    f.close()
    f = open(saveDir + "prefix", "w+")
    f.write(prefix)
    f.close()

if TBOARD_LOG:
    lossSum = tf.summary.scalar("train_loss", m.loss)
    summWriter = tf.summary.FileWriter(ckptDir + runId + "/train")
    validWriter = tf.summary.FileWriter(ckptDir + runId + "/validation")

init = tf.global_variables_initializer()

#-----------------------------MODEL TRAINING------------------------------------

sess = tf.Session()
sess.run(init)

# step, loss
minLoss = (0,100) 

epochCount = 0
for step in range(trainingSteps):
    batchX, batchY, batchId = training.next(batchSize)
    sess.run(m.optimize, feed_dict={_data: batchX, _labels: batchY})
    if batchId == trainMaxIdx or step == 1:
        # end of epoch
        # calculate current loss on training data
        tLoss, loss = sess.run([lossSum, m.loss], 
                feed_dict={_data: batchX, _labels: batchY})
        # calculate validation loss
        validX, validY, _ = validation.next(batchSize)
        vloss, loss = sess.run([lossSum, m.loss], 
                feed_dict={_data: validX, _labels: validY})
        if loss < minLoss[1]: minLoss = (step, loss)

        if TBOARD_LOG:
            # log various data
             validWriter.add_summary(vloss, step)
             summWriter.add_summary(tLoss, step)
        if not step == 1: epochCount += 1
        print("Step " + str(step) + ", batch loss = " + "{:.4f}".format(loss))

        if epochCount % epochsToSave == 0 and SAVE_CKPT:
            save = saver.save(sess, saveDir + "/" + str(step) + ".ckpt")
            f = open(saveDir + "latest", "w+")
            f.write(str(step))
            f.close()
            print("Saved checkpoint " + str(step))

    pretty.arrow(batchId, trainMaxIdx)

makePred()
os.mkdir(ckptDir + runId + "/mats")
m.saveMats(ckptDir + runId + "/mats/", sess)
if SAVE_CKPT:
    save = saver.save(sess, saveDir + "final.ckpt")
    f = open(saveDir + "latest", "w+")
    f.write("final")
    f.close()

    print("Training complete; model saved in file %s" % save)
    print("Minimum loss:", str(minLoss))
    f = open(saveDir + dataFile, 'a')
    f.write("# (step, loss)\n" + str(minLoss) + "\n")
    f.close()
sess.close()
